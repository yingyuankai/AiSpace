includes:
#  - "../base.yml"
  - "../pretrain/bert_huggingface.yml"
#  - "../pretrain/nezha.yml"
#  - "../pretrain/albert_chinese.yml"
#  - "../pretrain/electra_chinese.yml"
#  - "../pretrain/ernie.yml"
#  - "../pretrain/xlnet_chinese.yml"

model_name: bert_for_classification

model_attributes:
    hidden_dropout_prob: 0.5
    initializer_range: 0.02
    hidden_size: 1024
#    vocab_size: 21128
#    filters:
#      - 2
#      - 3
#      - 4
#    windows:
#      - 100
#      - 100
#      - 100

training:
#  policy:
#    name: "k-fold"
#    config:
#      k: 5
  learning_rate: 1e-5
  max_epochs: 30
  batch_size: 32

#  optimizer:
#    name: adam

  callbacks:
    # callback name
    early_stopping:
      switch: true
      config:
        patience: 2
    lr_finder:
      switch: false
      config:
        end_lr: 1e-4

  optimizer_wrappers:
    swa:
      switch: false
      config:
        start_epoch: 5
    lr_multiplier:
      switch: false
      config:
        multipliers:
          bert_for_seq_classification/bert: 0.1


dataset:
  name: glue_zh/tnews
  data_dir: "./data"
  transformer: "glue_zh/tnews"

  source:
    train: "train[:80%]"
    validation: "train[-20%:]"
    test: "validation"

  tokenizer:
#      name: bert_tokenizer
#      vocab:
#          filename: "/search/data1/yyk/data/pretrained/nezha/NEZHA-Base/vocab.txt"
#          special_tokens:
#              PAD: "[PAD]"
#              UNK: "[UNK]"
#              SEP: "[SEP]"
#              CLS: "[CLS]"
#              MASK: "[MASK]"
#      tokenize_chinese_chars: True
#      do_lower_case: True
#      do_basic_tokenize: True
#      non_split_tokens: null
      max_len: 100

  inputs:
    - name: input_ids
      column: input_ids
      type: LIST_OF_INT
      max_len: 100
    - name: token_type_ids
      column: token_type_ids
      type: LIST_OF_INT
      max_len: 100
    - name: attention_mask
      column: attention_mask
      type: LIST_OF_INT
      max_len: 100

  outputs:
    - name: output_1
      column: label
      type: CLASSLABEL
      num: 15
      labels: ["news_story", "news_culture", "news_entertainment", "news_sports", "news_finance",
               "news_house", "news_car", "news_edu", "news_tech", "news_military", "news_travel",
               "news_world", "news_stock", "news_agriculture", "news_game"]
      loss:
        name: sparse_categorical_crossentropy
        config:
          from_logits: true
      metrics:
        - name: sparse_categorical_accuracy
        - name: sparse_f1_score
          config:
            name: "macro_f1"
            num_classes: 15
            average: "macro"

pretrained:
#    name: ERNIE_1.0_max-len-512
#    name: chinese_xlnet_mid
#    name: chinese_electra_base
#    name: NEZHA-Base
    name: bert-base-chinese-huggingface
#    name: albert_xlarge_zh_google
    init_from_pretrained: true
#    config:
#      use_task_id: false


