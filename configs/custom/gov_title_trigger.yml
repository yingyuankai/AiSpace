includes:
  - "../pretrain/ernie.yml"

model_name: bert_for_ner

model_attributes:
    hidden_dropout_prob: 0.5
    initializer_range: 0.02
    hidden_size: 1024

training:
#  policy:
#    name: "k-fold"
#    config:
#      k: 5
  learning_rate: 1e-5
  max_epochs: 30
  batch_size: 8

#  optimizer:
#    name: adam

  callbacks:
    # callback name
    early_stopping:
      switch: true
      config:
        patience: 2
    lr_finder:
      switch: false
      config:
        end_lr: 1e-4

  optimizer_wrappers:
    swa:
      switch: false
      config:
        start_epoch: 5

dataset:
  name: gov_title/trigger
  data_dir: "./data"
  transformer: "gov_title/trigger"

  source:
    train: "train[:80%]"
    validation: "train[80%:90%]"
    test: "train[-10%:]"

  tokenizer:
    max_len: 512

  inputs:
    - name: input_ids
      column: input_ids
      type: LIST_OF_INT
      max_len: 512
    - name: token_type_ids
      column: token_type_ids
      type: LIST_OF_INT
      max_len: 512
    - name: attention_mask
      column: attention_mask
      type: LIST_OF_INT
      max_len: 512

  outputs:
    - name: output_1
      column: label
      type: LIST_OF_CLASSLABEL
      task: NER
      num: 3
      labels: ["B", "I", "O"]
      loss:
        name: myself_crf_loss
#        name: sparse_categorical_crossentropy
#        config:
#          from_logits: false
      metrics:
        - name: sparse_categorical_accuracy
        - name: sparse_f1_score
          config:
            name: "macro_f1"
            num_classes: 3
            average: "macro"

pretrained:
    name: ERNIE_1.0_max-len-512
    init_from_pretrained: true
#    config:
#      layers:
#        start: 0
#        end: 4
#        step: 1


