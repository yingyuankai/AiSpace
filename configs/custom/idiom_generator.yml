includes:
  - "../pretrain/gpt.yml"
  - "../generation/text_generation.yml"

model_name: bert_for_text_generation

model_attributes:
    hidden_dropout_prob: 0.5
    initializer_range: 0.02
    hidden_size: 1024

generation_attributes:
  do_sample: true
  num_beams: 3
  temperature: 0.7
  repetition_penalty: 2.0
  num_return_sequences: 1
  max_length: 300
  early_stopping: true

training:
#  policy:
#    name: "k-fold"
#    config:
#      k: 5
  learning_rate: 1e-5
  max_epochs: 1
  batch_size: 4
  do_eval: false
#  optimizer:
#    name: adam

  callbacks:
    # callback name
    early_stopping:
      switch: true
      config:
        patience: 2
    lr_finder:
      switch: false
      config:
        end_lr: 1e-4

  optimizer_wrappers:
    swa:
      switch: false
      config:
        start_epoch: 5

dataset:
  name: idiom/idiom_generator
  data_dir: "./data"
  transformer: "idiom/idiom_generator"

  source:
    train: "train[:80%]"
    validation: "train[80%:90%]"
    test: "train[-10%:]"

  tokenizer:
    max_len: 200

  inputs:
    - name: input_ids
      column: input_ids
      type: LIST_OF_INT
      max_len: 200
#    - name: token_type_ids
#      column: token_type_ids
#      type: LIST_OF_INT
#      max_len: 10
    - name: attention_mask
      column: attention_mask
      type: LIST_OF_INT
      max_len: 200

  outputs:
    - name: output_1
      column: label
      type: LIST_OF_CLASSLABEL
      num: 30000
      labels: vocab
      loss:
        name: sparse_categorical_crossentropy
        config:
          from_logits: false
      metrics:
        - name: sparse_categorical_accuracy
        - name: sparse_f1_score
          config:
            name: "macro_f1"
            num_classes: 30000
            average: "macro"

pretrained:
    name: CPM-LM-TF2
    init_from_pretrained: true
    config:
      layers:
        start: 0
        end: 4
        step: 1