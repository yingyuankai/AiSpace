generation_attributes:
  # The maximum length of the sequence to be generated.
  max_length: 20
  # The minimum length of the sequence to be generated.
  min_length: 10
  # Whether or not to use sampling ; use greedy decoding otherwise.
  do_sample: false
  # Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.
  early_stopping: false
  # Number of beams for beam search. 1 means no beam search.
  num_beams: 1
  # The value used to module the next token probabilities.
  temperature: 1.0
  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
  top_k: 50
  # If set to float < 1, only the most probable tokens with probabilities that add up to ``top_p`` or
  #    higher are kept for generation.
  top_p: 1.0
  # The parameter for repetition penalty. 1.0 means no penalty. See `this paper
  #    <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.
  repetition_penalty: 1.0
  # The id of the `padding` token.
  pad_token_id: None
  # The id of the `beginning-of-sequence` token.
  bos_token_id: None
  # The id of the `end-of-sequence` token.
  eos_token_id: None
  # Exponential penalty to the length. 1.0 means no penalty.
  #
  #    Set to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in
  #    order to encourage the model to produce longer sequences.
  length_penalty: 1.0
  # If set to int > 0, all ngrams of that size can only occur once.
  no_repeat_ngram_size: 0
  # List of token ids that are not allowed to be generated. In order to get the tokens of the words that
  #    should not appear in the generated text, use :obj:`tokenizer.encode(bad_word, add_prefix_space=True)`.
  bad_words_ids: []
  # The number of independently computed returned sequences for each element in the batch.
  num_return_sequences: 1
  # Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for
  #    tokens that are not masked, and 0 for masked tokens.
  #
  #    If not provided, will default to a tensor the same shape as :obj:`input_ids` that masks the pad token.
  #
  #    `What are attention masks? <../glossary.html#attention-mask>`__
  attention_mask: None
  # If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.
  decoder_start_token_id: None
  # Whether or not the model should use the past last key/values attentions (if applicable to the model) to
  #    speed up decoding.
  use_cache: true
